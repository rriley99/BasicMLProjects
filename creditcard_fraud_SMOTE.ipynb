{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This workbook will cover tackling the credit card fraud dataset @https://www.kaggle.com/mlg-ulb/creditcardfraud/download\\nThis will cover several parts:\\n1. Data Exploration and Data Cleaning\\n2. Modeling and Evaluating without Resampling\\n3. Resampling\\n4. Modeling\\n5. Evaluationw\\n\\nIt is important to read the dataset description on Kaggle, especially for this set:\\n\"It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to \\nconfidentiality issues, we cannot provide the original features and more background information about the data. \\nFeatures V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed \\nwith PCA are \\'Time\\' and \\'Amount\\'. Feature \\'Time\\' contains the seconds elapsed between each transaction and the first \\ntransaction in the dataset. The feature \\'Amount\\' is the transaction Amount, this feature can be used for example-dependant \\ncost-senstive learning. Feature \\'Class\\' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\"\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"This workbook will cover tackling the credit card fraud dataset @https://www.kaggle.com/mlg-ulb/creditcardfraud/download\n",
    "This will cover several parts:\n",
    "1. Data Exploration and Data Cleaning\n",
    "2. Modeling and Evaluating without Resampling\n",
    "3. Resampling\n",
    "4. Modeling\n",
    "5. Evaluationw\n",
    "\n",
    "It is important to read the dataset description on Kaggle, especially for this set:\n",
    "\"It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to \n",
    "confidentiality issues, we cannot provide the original features and more background information about the data. \n",
    "Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed \n",
    "with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first \n",
    "transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant \n",
    "cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "                Time            V1            V2            V3            V4  \\\n",
      "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean    94813.859575  1.165980e-15  3.416908e-16 -1.373150e-15  2.086869e-15   \n",
      "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
      "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
      "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
      "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
      "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
      "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
      "\n",
      "                 V5            V6            V7            V8            V9  \\\n",
      "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean   9.604066e-16  1.490107e-15 -5.556467e-16  1.177556e-16 -2.406455e-15   \n",
      "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
      "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
      "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
      "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
      "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
      "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
      "\n",
      "       ...           V21           V22           V23           V24  \\\n",
      "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
      "mean   ...  1.656562e-16 -3.444850e-16  2.578648e-16  4.471968e-15   \n",
      "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
      "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
      "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
      "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
      "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
      "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
      "\n",
      "                V25           V26           V27           V28         Amount  \\\n",
      "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
      "mean   5.340915e-16  1.687098e-15 -3.666453e-16 -1.220404e-16      88.349619   \n",
      "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
      "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
      "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
      "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
      "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
      "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
      "\n",
      "               Class  \n",
      "count  284807.000000  \n",
      "mean        0.001727  \n",
      "std         0.041527  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         1.000000  \n",
      "\n",
      "[8 rows x 31 columns]\n",
      "[[-0.69424232 -0.04407492  1.6727735  ...  0.33089162 -0.06378115\n",
      "   0.24496426]\n",
      " [ 0.60849633  0.16117592  0.1097971  ... -0.02225568  0.04460752\n",
      "  -0.34247454]\n",
      " [-0.69350046 -0.81157783  1.16946849 ... -0.13713686 -0.18102083\n",
      "   1.16068593]\n",
      " ...\n",
      " [ 0.98002374 -0.18243372 -2.14320514 ...  0.01103672 -0.0804672\n",
      "  -0.0818393 ]\n",
      " [-0.12275539  0.32125034  0.46332013 ...  0.26960398  0.31668678\n",
      "  -0.31324853]\n",
      " [-0.27233093 -0.11489898  0.46386564 ... -0.00598394  0.04134999\n",
      "   0.51435531]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Data set is pretty clean, the classes are already binary, the features are pretty standard and close together, \\nthere is little need for much preprocessing, let's just scale the data.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1. Data Exploration and Data Cleaning\n",
    "from pandas import read_csv\n",
    "from numpy import array\n",
    "from matplotlib import pyplot\n",
    "import seaborn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#load csv\n",
    "data = read_csv('creditcard.csv')\n",
    "\n",
    "#peak at data\n",
    "print(data.head()) \n",
    "\n",
    "#use df.describe\n",
    "print(data.describe(include='all'))\n",
    "\n",
    "#plot the classes and view the imbalance.\n",
    "pyplot.figure(figsize=(20,2))\n",
    "seaborn.countplot(y=data['Class'])\n",
    "\n",
    "\n",
    "#pre-processing\n",
    "data = data.drop(['Time'], axis = 1)\n",
    "\n",
    "X = array(data.loc[:, data.columns != 'Class'])\n",
    "y = array(data.loc[:, data.columns == 'Class']).reshape(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print(X)\n",
    "\"\"\"Data set is pretty clean, the classes are already binary, the features are pretty standard and close together, \n",
    "there is little need for much preprocessing, let's just scale the data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Modeling and Evaluating without Resampling\n",
    "\"\"\"Below we will be creating a LogisticRegression model with the data as is, as a control model of sorts.\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(trainX, trainy.ravel())\n",
    "yhat = model.predict(testX)\n",
    "\n",
    "print('Accuracy: %.3f' % accuracy_score(testy, yhat))\n",
    "print('Precision: %.3f' % precision_score(testy, yhat))\n",
    "print('Recall: %.3f' % recall_score(testy, yhat))\n",
    "print('F-measure: %.3f' % f1_score(testy, yhat))\n",
    "\n",
    "\"\"\"As expected we have a very high accuracy, but the precision and recall say otherwise. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Resampling\n",
    "\"\"\"We can see from the performance above that the model wasn't all that great. And as expected it is because the data is\n",
    "very unbalanced. Below we will view the data again and use the SMOTEENN method to resample the data to improve balance. \n",
    "NOTE: This takes some time to complete.\n",
    "\"\"\"\n",
    "\n",
    "# We know this is an imbalanced dataset, so let's see how imbalanced.\n",
    "print('% breakdown:')\n",
    "print(data['Class'].value_counts() / len(data))\n",
    "\n",
    "# above we can see that our data is 99.8273% non-fraud cases. This means that probably any classifier would score pretty \n",
    "# well by simply marking every case as non-fraudulent.\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# define sampling strategy\n",
    "sample = SMOTEENN(sampling_strategy=0.25)\n",
    "\n",
    "# fit and apply the transform\n",
    "X_smoteenn, y_smoteenn = sample.fit_resample(X, y)\n",
    "\n",
    "# summarize class distribution\n",
    "print('Resampled % breakdown:')\n",
    "print(y_smoteenn / len(y_smoteenn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(X_smoteenn, y_smoteenn, test_size=0.5, stratify=y_smoteenn)\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(trainX, trainy)\n",
    "yhat = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Evaluation\n",
    "\"\"\"We will look at some run-of-the-mill evauluation, and then an evaluation using the Fbeta-measure.\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('Accuracy: %.3f' % accuracy_score(testy, yhat))\n",
    "print('Precision: %.3f' % precision_score(testy, yhat))\n",
    "print('Recall: %.3f' % recall_score(testy, yhat))\n",
    "print('F-measure: %.3f' % f1_score(testy, yhat))\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "print('Fbeta-measure(beta=1): %.3f' % fbeta_score(testy, yhat, beta=1))\n",
    "print('Fbeta-measure(beta=0.5): %.3f' % fbeta_score(testy, yhat, beta=0.5))\n",
    "print('Fbeta-measure(beta=2): %.3f' % fbeta_score(testy, yhat, beta=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.000\n",
      "Precision: 0.952\n",
      "Recall: 0.801\n",
      "F-measure: 0.870\n"
     ]
    }
   ],
   "source": [
    "# An Appendix of sorts\n",
    "\"\"\"I read an article that said decision tree style classifiers do a pretty good job on imbalanced data, so I thought I'd add\n",
    "this on so we could find out. \n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "model = RandomForestClassifier()\n",
    "model.fit(trainX, trainy.ravel())\n",
    "yhat = model.predict(testX)\n",
    "\n",
    "print('Accuracy: %.3f' % accuracy_score(testy, yhat))\n",
    "print('Precision: %.3f' % precision_score(testy, yhat))\n",
    "print('Recall: %.3f' % recall_score(testy, yhat))\n",
    "print('F-measure: %.3f' % f1_score(testy, yhat))\n",
    "\n",
    "\"\"\"So as we can see, it did a pretty good job, the recall is good, and with some tuning, this might be a decent way to tackle\n",
    "an imbalanced dataset. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
