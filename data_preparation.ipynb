{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Working through the 'Mini-Course' from MachineLearningMastery: https://machinelearningmastery.com/data-preparation-for-machine-learning-7-day-mini-course/\\nThis notebook will cover all lessons:\\nLesson 01: Importance of Data Preparation.\\nLesson 02: Fill Missing Values With Imputation.\\nLesson 03: Select Features With RFE.\\nLesson 04: Scale Data With Normalization.\\nLesson 05: Transform Categories With One Hot Encoding.\\nLesson 06: Transform Numbers to Categories With kBins.\\nLesson 07: Dimensionality Reduction with PCA.\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Working through the 'Mini-Course' from MachineLearningMastery: https://machinelearningmastery.com/data-preparation-for-machine-learning-7-day-mini-course/\n",
    "This notebook will cover all lessons:\n",
    "Lesson 01: Importance of Data Preparation.\n",
    "Lesson 02: Fill Missing Values With Imputation.\n",
    "Lesson 03: Select Features With RFE.\n",
    "Lesson 04: Scale Data With Normalization.\n",
    "Lesson 05: Transform Categories With One Hot Encoding.\n",
    "Lesson 06: Transform Numbers to Categories With kBins.\n",
    "Lesson 07: Dimensionality Reduction with PCA.\n",
    "\"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lesson 01: Importance of Data Preparation.\n",
    "\"\"\"Why do we need to prep the data?\n",
    "Data Types: Machine learning algorithms require data to be numbers.\n",
    "Data Requirements: Some machine learning algorithms impose requirements on the data.\n",
    "Data Errors: Statistical noise and errors in the data may need to be corrected.\n",
    "Data Complexity: Complex nonlinear relationships may be teased out of the data.\n",
    "\n",
    "Methods of data preparation:\n",
    "Data Cleaning: Identifying and correcting mistakes or errors in the data.\n",
    "Feature Selection: Identifying those input variables that are most relevant to the task.\n",
    "Data Transforms: Changing the scale or distribution of variables.\n",
    "Feature Engineering: Deriving new variables from available data.\n",
    "Dimensionality Reduction: Creating compact projections of the data.\n",
    "\n",
    "Examples of data preparation:\n",
    "1. Scaling and normalization of data.\n",
    "2. Coverting a gender column to a boolean column.\n",
    "3. Looking for highly correlated features, that prove to be redundant or insignificant.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 1605\n",
      "Missing: 0\n"
     ]
    }
   ],
   "source": [
    "#Lesson 02: Fill Missing Values With Imputation.\n",
    "\"\"\"For most data, if a feature is null in a row, the row will be useless. The SimpleImputer class from sk-learn\n",
    "transforms all missing values marked with a NaN value with the mean of the column. Other options are median, mode and \n",
    "a user-defined constant.\"\"\"\n",
    "from numpy import isnan\n",
    "from pandas import read_csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load dataset and fill na with '?'\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "dataframe = read_csv(url, header=None, na_values='?')\n",
    "\n",
    "# split into input and output elements\n",
    "data = dataframe.values\n",
    "ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "X, y = data[:, ix], data[:, 23]\n",
    "\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(X).flatten()))\n",
    "\n",
    "# define imputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# fit on the dataset\n",
    "imputer.fit(X)\n",
    "\n",
    "# transform the dataset\n",
    "Xtrans = imputer.transform(X)\n",
    "\n",
    "# print total missing\n",
    "print('Missing: %d' % sum(isnan(Xtrans).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: 0, Selected=False, Rank: 3\n",
      "Column: 1, Selected=False, Rank: 4\n",
      "Column: 2, Selected=True, Rank: 1\n",
      "Column: 3, Selected=True, Rank: 1\n",
      "Column: 4, Selected=True, Rank: 1\n",
      "Column: 5, Selected=False, Rank: 5\n",
      "Column: 6, Selected=True, Rank: 1\n",
      "Column: 7, Selected=True, Rank: 1\n",
      "Column: 8, Selected=True, Rank: 1\n",
      "Column: 9, Selected=False, Rank: 2\n"
     ]
    }
   ],
   "source": [
    "#Lesson 03: Select Features With RFE.\n",
    "\"\"\"Feature selection is the process of reducing the number of input variables when developing a predictive model. Reducing \n",
    "input variables can reduce cost and even improve results. Recursive Feature Elimination (RFE) is easy to configure and use.\n",
    "\"\"\"\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define dataset with 5 redundant features\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "\n",
    "# define RFE using a Decision-Tree and 5 selected features.\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n",
    "\n",
    "# fit RFE\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# summarize all features\n",
    "for i in range(X.shape[1]):\n",
    "    print('Column: %d, Selected=%s, Rank: %d' % (i, rfe.support_[i], rfe.ranking_[i]))\n",
    "    \n",
    "\"\"\"This lesson seems a little lack luster. It would be nice to know when RFE is a good choice and how to pick the \n",
    "estimator or the features to select.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.39324489 -5.77732048 -0.59062319 -2.08095322  1.04707034]\n",
      " [-0.45820294  1.94683482 -2.46471441  2.36590955 -0.73666725]\n",
      " [ 2.35162422 -1.00061698 -0.5946091   1.12531096 -0.65267587]]\n",
      "[[0.77608466 0.0239289  0.48251588 0.18352101 0.59830036]\n",
      " [0.40400165 0.79590304 0.27369632 0.6331332  0.42104156]\n",
      " [0.77065362 0.50132629 0.48207176 0.5076991  0.4293882 ]]\n"
     ]
    }
   ],
   "source": [
    "#Lesson 04: Scale Data With Normalization.\n",
    "\"\"\"Algorithms that use a weighted sum of the input (linear regression) and algorithms that use distance measures (KNN) \n",
    "benefit from normalization of feature values. A good additional rule of thumb is that if your data is near gaussian, use \n",
    "Standardization. Normalization otherwise or when you plan on using either of the aforementioned algos.\"\"\"\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=5, n_redundant=0, random_state=1)\n",
    "\n",
    "# summarize data before the transform\n",
    "print(X[:3, :])\n",
    "\n",
    "# define the scaler\n",
    "trans = MinMaxScaler()\n",
    "\n",
    "# transform the data\n",
    "X_norm = trans.fit_transform(X)\n",
    "\n",
    "# summarize data after the transform\n",
    "print(X_norm[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"'40-49'\" \"'premeno'\" \"'15-19'\" \"'0-2'\" \"'yes'\" \"'3'\" \"'right'\"\n",
      "  \"'left_up'\" \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'15-19'\" \"'0-2'\" \"'no'\" \"'1'\" \"'right'\" \"'central'\"\n",
      "  \"'no'\"]\n",
      " [\"'50-59'\" \"'ge40'\" \"'35-39'\" \"'0-2'\" \"'no'\" \"'2'\" \"'left'\" \"'left_low'\"\n",
      "  \"'no'\"]]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Lesson 05: Transform Categories With One Hot Encoding.\n",
    "\"\"\"Here we will look at dealing with categorical data, yes/no or red/green/blue. Sk-learn has a OneHotEncoder class that will\n",
    "work for us. The OneHotEncoder will convert a categorical column \"color\" to three columns, booleans, like \"isgreen\", \"isred\"\n",
    "or \"isblue. \"\n",
    "\"\"\"\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# define the location of the dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv\"\n",
    "\n",
    "# load the dataset\n",
    "dataset = read_csv(url, header=None)\n",
    "\n",
    "# retrieve the array of data\n",
    "data = dataset.values\n",
    "\n",
    "# separate into input and output columns\n",
    "X = data[:, :-1].astype(str)\n",
    "y = data[:, -1].astype(str)\n",
    "\n",
    "# summarize the raw data\n",
    "print(X[:3, :])\n",
    "\n",
    "# define the one hot encoding transform\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# fit and apply the transform to the input data\n",
    "X_oe = encoder.fit_transform(X)\n",
    "\n",
    "# summarize the transformed data\n",
    "print(X_oe[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.39324489 -5.77732048 -0.59062319 -2.08095322  1.04707034]\n",
      " [-0.45820294  1.94683482 -2.46471441  2.36590955 -0.73666725]\n",
      " [ 2.35162422 -1.00061698 -0.5946091   1.12531096 -0.65267587]]\n",
      "[[7. 0. 4. 1. 5.]\n",
      " [4. 7. 2. 6. 4.]\n",
      " [7. 5. 4. 5. 4.]]\n"
     ]
    }
   ],
   "source": [
    "#Lesson 06: Transform Numbers to Categories With kBins\n",
    "\"\"\"Naturally the next lesson is to do the reverse operation. Some algos, like some decision tree and \n",
    "rule-based algorithms prefer categorical data. The process of discretization puts values in probabilistic bins.\n",
    "The sk-learn class KBinsDiscretizer will do this for us. This algorithm can \"discretize\" multiple ways, by a \n",
    "uniform distribution, a quantile distribution or by a k-means clustering operation. \"\"\"\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=5, n_redundant=0, random_state=1)\n",
    "\n",
    "# summarize data before the transform\n",
    "print(X[:3, :])\n",
    "\n",
    "# define the transform with 10 bins, integer bins and a uniform distribution.\n",
    "trans = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "\n",
    "# transform the data\n",
    "X_discrete = trans.fit_transform(X)\n",
    "\n",
    "# summarize data after the transform\n",
    "print(X_discrete[:3, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.53448246  0.93837451  0.38969914  0.0926655   1.70876508  1.14351305\n",
      "  -1.47034214  0.11857673 -2.72241741  0.2953565 ]\n",
      " [-2.42280473 -1.02658758 -2.34792156 -0.82422408  0.59933419 -2.44832253\n",
      "   0.39750207  2.0265065   1.83374105  0.72430365]\n",
      " [-1.83391794 -1.1946668  -0.73806871  1.50947233  1.78047734  0.58779205\n",
      "  -2.78506977 -0.04163788 -1.25227833  0.99373587]]\n",
      "[[-1.64710578 -2.11683302  1.98256096]\n",
      " [ 0.92840209  4.8294997   0.22727043]\n",
      " [-3.83677757  0.32300714  0.11512801]]\n"
     ]
    }
   ],
   "source": [
    "#Lesson 07: Dimensionality Reduction With PCA (Feature Extraction)\n",
    "\"\"\"Generally speaking, more input features make modeling harder, in the same sense that more ingredients in a recipe can \n",
    "make it more complex and difficult. The sk-learn class PCA uses Principal Component Analysis (throw back to MATH 415 @ UIUC).\n",
    "The case for PCA is the situation when you have many features after feature selection, PCA will generate a number \n",
    "(n_components) of eigenvectors.\n",
    "\"\"\"\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=7, random_state=1)\n",
    "\n",
    "# summarize data before the transform\n",
    "print(X[:3, :])\n",
    "\n",
    "# define the transform\n",
    "trans = PCA(n_components=3)\n",
    "\n",
    "# transform the data\n",
    "X_dim = trans.fit_transform(X)\n",
    "\n",
    "# summarize data after the transform\n",
    "print(X_dim[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
