{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we will be taking a look at the breast cancer dataset, and will be writing an Support Vector Machine Algo from \"scratch\".\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 columns dropped\n",
      "training started...\n",
      "Epoch is:1 and Cost is: 7226.382984689499\n",
      "Epoch is:2 and Cost is: 6582.151387643106\n",
      "Epoch is:4 and Cost is: 5492.0023941922145\n",
      "Epoch is:8 and Cost is: 3859.169792319114\n",
      "Epoch is:16 and Cost is: 2667.946213857656\n",
      "Epoch is:32 and Cost is: 1976.6726887131326\n",
      "Epoch is:64 and Cost is: 1593.5051552774753\n",
      "Epoch is:128 and Cost is: 1326.9398558326545\n",
      "Epoch is:256 and Cost is: 1160.05674083452\n",
      "Epoch is:512 and Cost is: 1080.8734052929533\n",
      "Epoch is:1024 and Cost is: 1048.8004903259514\n",
      "Epoch is:2048 and Cost is: 1042.7716240571858\n",
      "training finished.\n",
      "accuracy on test dataset: 0.9824561403508771\n",
      "recall on test dataset: 0.9534883720930233\n",
      "precision on test dataset: 0.9534883720930233\n"
     ]
    }
   ],
   "source": [
    "# The Cost Function: An SVM seeks to minimize this function. We are seeking a hyperplane (n-dimension surface)\n",
    "# that seperates the postive cases from the negative cases while minimizing misclassification. The cost function is \n",
    "# more or less proportional to the quality of the model. The ruling equation is J(w) ~ w^2 + (C/N)*nSUMi[1-yi(w*xi+b)] \n",
    "\n",
    "def compute_cost(W, X, Y):\n",
    "    # Here we will attempt to compute the cost by starting with the hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - Y * (np.dot(X,W))\n",
    "    distances[distances < 0] = 0\n",
    "    hinge_loss = reg_strength * (np.sum(distances) / N)\n",
    "\n",
    "    cost = 1/2 * np.dot(W, W) + hinge_loss\n",
    "\n",
    "    return cost\n",
    "\n",
    "# The Gradient of J(w) ~(1/N)*nSUMi[w when 1-yi(w*xi+b), else w-C{yi,xi}]\n",
    "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
    "    if type(Y_batch) == np.float64:\n",
    "        Y_batch = np.array([Y_batch])\n",
    "        X_batch = np.array([X_batch])\n",
    "    \n",
    "    distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
    "    dw = np.zeros(len(W))\n",
    "    \n",
    "    for ind, d in enumerate(distance):\n",
    "        if max(0, d) == 0:\n",
    "            di = W\n",
    "        else:\n",
    "            di = W - (reg_strength * Y_batch[ind] * X_batch[ind])\n",
    "        \n",
    "        dw += di\n",
    "    \n",
    "    dw = dw/len(Y_batch)  # average\n",
    "    \n",
    "    return dw\n",
    "\n",
    "# Training using Stochastic Gradient Descent (SGD), \n",
    "# we will seek to minimize J(w) which means minimize w^2 and minimize hinge-loss. \n",
    "# We will do so by moving opposite of the gradient (this is the Descent part) (w') until convergence w' = min(J(w))\n",
    "def sgd(features, outputs):\n",
    "    max_epochs = 5000\n",
    "    weights = np.zeros(features.shape[1])\n",
    "    nth = 0\n",
    "    prev_cost = float(\"inf\")\n",
    "    cost_threshold = 0.01 #percentage\n",
    "    \n",
    "    for epoch in range(1, max_epochs):\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        \n",
    "        for ind, x in enumerate(X):\n",
    "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
    "            weights = weights - (learning_rate * ascent)\n",
    "        \n",
    "        # convergence check on 2^nth epoch\n",
    "        if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
    "            cost = compute_cost(weights, features, outputs)\n",
    "            print(\"Epoch is:{} and Cost is: {}\".format(epoch, cost))\n",
    "            # stoppage criterion\n",
    "            if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "                return weights\n",
    "            prev_cost = cost\n",
    "            nth += 1    \n",
    "    return weights\n",
    "\n",
    "# V2:\n",
    "def remove_correlated_features(X):\n",
    "    corr_threshold = 0.9\n",
    "    corr = X.corr()\n",
    "    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n",
    "    \n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= corr_threshold:\n",
    "                drop_columns[j] = True\n",
    "    \n",
    "    columns_dropped = X.columns[drop_columns]\n",
    "    X.drop(columns_dropped, axis=1, inplace=True)\n",
    "    \n",
    "    return columns_dropped\n",
    "\n",
    "# V2:\n",
    "def remove_less_significant_features(X, Y):\n",
    "    # define a Significance Level \"sl\"\n",
    "    sl = 0.05\n",
    "    regression_ols = None\n",
    "    columns_dropped = np.array([])\n",
    "    \n",
    "    for itr in range(0, len(X.columns)):\n",
    "        regression_ols = sm.OLS(Y, X).fit()\n",
    "        max_col = regression_ols.pvalues.idxmax()\n",
    "        max_val = regression_ols.pvalues.max()\n",
    "        if max_val > sl:\n",
    "            X.drop(max_col, axis='columns', inplace=True)\n",
    "            columns_dropped = np.append(columns_dropped, [max_col])\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    regression_ols.summary()\n",
    "    \n",
    "    return columns_dropped\n",
    "\n",
    "def init():\n",
    "    # Explore the dataset\n",
    "    data = pd.read_csv('breast_cancer_data.csv')\n",
    "    data.describe()\n",
    "    \n",
    "    # Pre-Processing and Feature Engineering:\n",
    "    # Let's do some pre-processing and remove the last col, and the ids. We should also convert the diagnosis to ints\n",
    "    data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n",
    "    diagnosis = {'M':1.0, 'B':-1.0}\n",
    "    data['diagnosis'] = data['diagnosis'].map(diagnosis)\n",
    "\n",
    "    # Let's split the data up into features and labels, and normalize the features.\n",
    "    Y = data.loc[:, 'diagnosis']  # labels\n",
    "    X = data.iloc[:, 1:] # features\n",
    "    \n",
    "    # V2: filter features\n",
    "    columns_dropped = remove_correlated_features(X)\n",
    "    columns_dropped = remove_less_significant_features(X, Y)\n",
    "    print(f'{len(columns_dropped)} columns dropped')\n",
    "    \n",
    "    # Normalize the features using MinMaxScalar from sklearn.preprocessing\n",
    "    X_normalized = MinMaxScaler().fit_transform(X.values)\n",
    "    X = pd.DataFrame(X_normalized)\n",
    "    X.head()\n",
    "\n",
    "    # Add the intercept col (b) and split the data (train/test/validate : 70/20/10)\n",
    "    X.insert(loc=len(X.columns), column='intercept', value=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Split once more for unseen data-subset\n",
    "    #X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.125, random_state=1)\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"training started...\")\n",
    "    W = sgd(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(\"training finished.\")\n",
    "    \n",
    "    # Test the model\n",
    "    y_test_predicted = np.array([])\n",
    "    for i in range(X_test.shape[0]):\n",
    "        yp = np.sign(np.dot(W, X_test.to_numpy()[i])) #model\n",
    "        y_test_predicted = np.append(y_test_predicted, yp)\n",
    "        \n",
    "    print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test.to_numpy(), y_test_predicted)))\n",
    "    print(\"recall on test dataset: {}\".format(recall_score(y_test.to_numpy(), y_test_predicted)))\n",
    "    print(\"precision on test dataset: {}\".format(recall_score(y_test.to_numpy(), y_test_predicted)))\n",
    "           \n",
    "# Send it \n",
    "reg_strength = 10000 \n",
    "learning_rate = 0.000001\n",
    "init()\n",
    "\n",
    "# This returned\n",
    "# accuracy on test dataset: 0.9736842105263158\n",
    "# recall on test dataset: 0.9285714285714286\n",
    "# precision on test dataset: 0.9285714285714286\n",
    "# time 5000 epochs\n",
    "# We will try to improve this by updating somethings above, which I will mark with comments \"V2:\",\n",
    "# we will seek to remove some correlated (linear dependence) features depending on their \"p-value\" which \n",
    "# more-or-less rate the importance of a feature in the variation of the dependent variable \"y\". We will be removing \n",
    "# features with high p-values, keeping them makes the model more complex than need be. The new functions are:\n",
    "# (1) remove_correlated_features()\n",
    "# (2) remove_less_significant_features()\n",
    "\n",
    "# Round 2 returned better results!\n",
    "# accuracy on test dataset: 0.9824561403508771\n",
    "# recall on test dataset: 0.9534883720930233\n",
    "# precision on test dataset: 0.9534883720930233\n",
    "# time 2048 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
